---
title: CNN结构演化
date: 2022-04-01 16:36:50
tags:
  - Machine Learning（机器学习）
  - Deep Learning（深度学习）
  - CNN（卷积神经网络）
categories:
  - Machine Learning（机器学习）
  - [Deep Learning（深度学习）, CNN（卷积神经网络）]
---
> http://www.cnblogs.com/yanshw/p/10609615.html

# CNN发展史
![](https://i.loli.net/2019/04/01/5ca1cd63e8cfe.jpg)

这是imageNet比赛的历史成绩

可以看到准确率越来越高，网络越来越深。

<!-- more -->

加深网络比加宽网络有效的多，这已是公认的结论。

## CNN结构演化图
![](https://i.loli.net/2019/04/01/5ca1cdbee96b0.jpg)

## AlexNet
诞生于2012年，因为当时用了两个GPU（硬件设备差），所以结构图是2组并行
![](https://i.loli.net/2019/04/01/5ca1ce5ad5dad.jpg)

网络结构总共8层，5个卷积层，3个全连接层，最后输出1000个分类

#### 分层结构图
![](https://i.loli.net/2019/04/01/5ca1ce724cd68.jpg)

简单解释如下：

conv1：输入为224x224x3，96个shape为11x11x3的卷积核，步长为4，输出55x55x96的特征图，(224-11)/4+1，paddig为valid，故等于55，relu，池化，池化视野3x3，步长为2，这个池化稍有不同，后面解释，然后，lrn，局部归一化，后面解释，输出27x27x96　　（这里输入的224在卷积时计算不方便，后来改成了227）

conv2 conv3 conv4 conv5基本类似

fc6：将输出6x6x256展开成一维9216，接4096个神经元，输出4096个值，dropout，还是4096个值

fc7：写的有些问题，第一个框内应该是4096，不打紧

fc8：输出1000个类别

#### 注意几点

1. 重叠池化：一般情况下，我们的池化视野和步长是一样的，但是这里池化视野大于步长，这样就有重叠，故称为重叠池化
2. 局部响应归一化：虽然relu不会梯度饱和，但是笔者认为lrn可以提高模型的泛化能力

#### AlexNet较之前网络的改进

1. 网络变深
2. relu激活函数
3. 数据增强

　　通俗理解就是准备更多更全的数据

　　// 法1：平移图像和水平翻转图像，从原始的256x256的图像中随机提取227x227的块，并且水平翻转，作为训练数据

　　// 法2：改变训练图像的RGB通道的强度。特别的，本文对整个ImageNet训练集的RGB像素值进行了PCA。对每一幅训练图像，本文加上多倍的主成分，倍数的值为相应的特征值乘以一个均值为0标准差为0.1的高斯函数产生的随机变量。

4. dropout

　　// 以一定的概率将神经元的输出置为0，就是忽略该神经元的存在，既不前向传递，也不反向传播，这是AlexNet很大的一个创新。

数据增强和dropout都是防止过拟合的。

## VGG

> Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.

诞生于2014年，imageNet2014的第二名

![](https://i.loli.net/2019/04/01/5ca1ceb15376c.jpg)

上图是VGG16--黑色框+蓝色框=16

====================================

![](https://i.loli.net/2019/04/01/5ca1ced443403.jpg)

上图为VGG19

====================================

 VGG共有6种结构，常用的是D和E，即VGG16和VGG19

VGG网络结构非常规则，共分为5组卷积，每组卷积都使用3x3的卷积核，卷积后都使用2x2的max池化，最后接3个全连接

可以看到C网络的最后3个卷积使用了1x1的卷积，这是什么意思呢?

1x1的卷积其实是对特征的线性变换，主要用来综合特征，这个方法后来被推广解决多种问题。

#### VGG的优点

1. 使用较小的卷积核，使得参数更少，网络更深

2. 卷积层的堆叠，这样做一可以保证感受视野，即2个3x3的感受野堆叠相当于一个5x5，二是经过多个非线性转换，提取特征的能力更强

3. VGG在训练时，先训练简单的如A网络，然后用A网络的参数（卷积和全连接）初始化后面的复杂网络，更快收敛

4. VGG认为lrn作用不大，去掉了lrn

VGG虽然网络更深，但比AlexNet收敛更快

#### VGG的缺点

占用内存较大

#### VGG的数据处理

1. 数据标准化
2. 数据增强采用Multi-Scale方法。将原始图像缩放到不同尺寸S，然后再随机裁切[224,224]的图片，这样能增加很多数据量，对于防止模型过拟合有很不错的效果。实践中，作者令S在[256,512]这个区间内取值，使用Multi-Scale获得多个版本的数据，并将多个版本的数据合在一起进行训练。

## GoogleNet
诞生于2014年，imageNet2014的第一名，和VGG一同出道，当VGG是第二名，但因其结构复杂，所以没有VGG更常用。

#### 结构图
![](https://i.loli.net/2019/04/01/5ca1d07c7e075.jpg)

网络太过复杂，不多解释了，这个网络估计也很少被用到。

但是 googlenet 有个很大的创新点，并且被后来很多网络借鉴，就是 global average pooling

global average pooling 一般放在网络最后，替代全连接层。

因为全连接层参数太多，占整个模型参数的比例可以高达90%左右，很难训练，而且参数太多，没有合适的正则化方法，很容易过拟合

global average pooling 做法很简单，就是对每个feature map取全局average，然后每个feature变成了一个数，然后最后接了一个 输出层fc网络

为什么这样，我以后会讲

![](https://i.loli.net/2019/04/01/5ca1d1065f7c3.jpg)

## ResNet

> He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

ResNet借鉴了上面的global average pooling

ResNet 又叫深度残差网络

#### 整体结构

Resnet50和Resnet101是其中最常用的网络结构

![](https://i.loli.net/2019/04/01/5ca1d3b0ce89e.jpg)

#### 深度网络的退化问题

深度网络难以训练，梯度消失，梯度爆炸，老生常谈，不多说

![](https://i.loli.net/2019/04/01/5ca1d1e1cc7dc.jpg)

ResNet 解决了这个问题，并且将网络深度扩展到了最多152层。怎么解决的呢？

#### 残差学习

结构如图：

![](https://i.loli.net/2019/04/01/5ca1d22a8e8c7.jpg)

在普通的卷积过程中加入了一个x的恒等映射（identity mapping）

专家把这称作 skip connections  或者 shortcut connections

为什么要这样呢？下面我从多个角度阐述这个问题。

##### 生活角度

每学习一个模型，我都希望能用日常的生活去解释为什么模型要这样，一是加深对模型的理解，二是给自己搭建模型寻找灵感，三是给优化模型寻找灵感。

resnet 无疑是解决很难识别的问题的，那我举一个日常生活中人类也难以识别的问题，看看这个模型跟人类的识别方法是否一致。

比如人类识别杯子里的水烫不烫

一杯水，我摸了一下，烫，好，我的神经开始运转，最后形成理论杯子里的水烫，这显然不对

又一杯水，我一摸，不烫，好嘛，这咋办，认知混乱了，也就是无法得到有效的参数，

那人类是怎么办呢？

我们不止是摸一摸，而且在摸过之后还要把杯子拿起来仔细看看，有什么细节可以帮助我们更好的识别，这就是在神经经过运转后，又把x整体输入，

当然即使我们拿起杯子看半天，也可能看不出任何规律来帮助我们识别，那人类的作法是什么呢？我记住吧，这种情况要小心，这就是梯度消失了，学习不到任何规律，记住就是恒等映射，

这个过程和resnet是一致的。

##### 网络结构角度

![](https://i.loli.net/2019/04/01/5ca1d29ae573b.jpg)

当梯度消失时，f(x)=0，y=g(x)=relu(x)=x，怎么理解呢？

1. 当梯度消失时，模型就是记住，长这样的就是该类别，是一个大型的过滤器

2. 在网络上堆叠这样的结构，就算梯度消失，我什么也学不到，我至少把原来的样子恒等映射了过去，相当于在浅层网络上堆叠了“复制层”，这样至少不会比浅层网络差。

3. 万一我不小心学到了什么，那就赚大了，由于我经常恒等映射，所以我学习到东西的概率很大。

##### 数学角度

![](https://i.loli.net/2019/04/01/5ca1d2cd609b2.jpg)

可以看到 有1 的存在，导数基本不可能为0

##### 那为什么叫残差学习呢

![](https://i.loli.net/2019/04/01/5ca1d2f76374b.jpg)

可以看到 F(x) 通过训练参数 得到了H(x)-x，也就是残差，所以叫残差学习，这比学习H(x)要简单的多。

#### 等效映射 identity mapping

上面提到残差学习中需要进行 F(x)+x，在resnet中，卷积都是 same padding 的，当通道数相同时，直接相加即可，

但是通道数不一样时需要寻求一种方法使得  y=f(x)+wx

实现w有两种方式

1. 直接补0
2. 通过使用多个 1x1 的卷积来增加通道数。

#### 网络结构——block

block为一个残差单元，resnet 网络由多个block 构成，resnet 提出了两种残差单元

![](https://i.loli.net/2019/04/01/5ca1d376ad89e.jpg)

左边针对的是ResNet34浅层网络，右边针对的是ResNet50/101/152深层网络，右边这个又被叫做 bottleneck

bottleneck 很好地减少了参数数量，第一个1x1的卷积把256维channel降到64维，第三个又升到256维，总共用参数：1x1x256x64+3x3x64x64+1x1x64x256=69632，

如果不使用 bottleneck，参数将是 3x3x256x256x2=1179648，差了16.94倍

这里的输出通道数是根据输入通道数确定的，因为要与x相加。

#### 与VGG比较

1. 与vgg相比，其参数少得多，因为vgg有3个全连接层，这需要大量的参数，而resnet用 avg pool 代替全连接，节省大量参数。
2. 参数少，残差学习，所以训练效率高

#### 结构参数

Resnet50和Resnet101是其中最常用的网络结构。

![](https://i.loli.net/2019/04/01/5ca1d41723188.jpg)

我们看到所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x

其结构是相对固定的，只是通道数根据输入确定。